---
layout: page
title: Resources for Applying Deep Learning in NLP
permalink: /dl/
---

[Update me on Github](https://github.com/acepor/acepor.github.io/blob/master/dl.md)

## Deep Learning Basics

### Basics

[Neural Networks, Manifolds, and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) [Note](#1)

[Deep Learning, NLP, and Representations](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)

[Calculus on Computational Graphs: Backpropagation](https://colah.github.io/posts/2015-08-Backprop/)

[Hacker's guide to Neural Networks](https://karpathy.github.io/neuralnets/)

### Hardware Configuration

[The $1700 great Deep Learning box: Assembly, setup and benchmarks](https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415)

## RNN

### Basics

[The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)

Recurrent Neural Networks Tutorial by [Denny Britz](http://www.wildml.com/about/)

1. [Introduction To RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)

2. [Implementing A Rnn With Python, Numpy And Theano](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)

3. [Backpropagation Through Time And Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)

4. [Implementing A Gru/Lstm Rnn With Python And Theano](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)

[Attention and Augmented Recurrent Neural Networks](http://distill.pub/2016/augmented-rnns/)

### LSTM

[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[Rnns In Tensorflow, A Practical Guide And Undocumented Features](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)

[Sequence Tagging with Tensorflow](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)

[A noobâ€™s guide to implementing RNN-LSTM using Tensorflow](http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/)

[The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)

## CNN

[Understanding Convolutions](https://colah.github.io/posts/2014-07-Understanding-Convolutions/)

[Conv Nets: A Modular Perspective](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/)

[Groups & Group Convolutions](https://colah.github.io/posts/2014-12-Groups-Convolution/)

[Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)

[Implementing A Cnn For Text Classification In Tensorflow](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)    

## Notes

#### 1

    It uses visualizations to explain the mechanics of how a low-dimensional deep neural network works, in order to provide a possible way to understand how a deeper network works.
